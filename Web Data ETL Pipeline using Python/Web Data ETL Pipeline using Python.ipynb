{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3de190b",
   "metadata": {},
   "source": [
    "**A Web Data ETL (Extract, Transform, Load) pipeline is a systematic process used in data engineering to collect, transform, and load data from various sources on the internet into a structured and usable format for analysis and storage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a684b70",
   "metadata": {},
   "source": [
    "### Now let’s see how to build a Web Data ETL pipeline using Python. Here I will create a pipeline to scrape textual data from any article on the web. We will aim to store the data about the frequencies of each word in the article. I’ll start this task by importing the necessary Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faaf029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in f:\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: nltk in f:\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: tqdm in f:\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in f:\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in f:\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in f:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc1b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TEKAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use command for installing beautifulsoup and nltk: pip install beautifulsoup4 nltk\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762ea64",
   "metadata": {},
   "source": [
    "### Now let’s start by extracting text from any article on the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e54d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def extract_article_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        article_text = soup.get_text()\n",
    "        return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e62a42",
   "metadata": {},
   "source": [
    "**In the above code, the WebScraper class provides a way to conveniently extract the main text content of an article from a given web page URL. By creating an instance of the WebScraper class and calling its extract_article_text method, we can retrieve the textual data of the article, which can then be further processed or analyzed as needed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f9b15",
   "metadata": {},
   "source": [
    "### As we want to store the frequency of each word in the article, we need to clean and preprocess the data. Here’s how we can clean and preprocess the text extracted from the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99aec9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, nltk_stopwords):\n",
    "        self.nltk_stopwords = nltk_stopwords\n",
    "\n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf88f0f",
   "metadata": {},
   "source": [
    "**So till now, we have defined classes for scraping and preparing the data.** \n",
    "### Now we need to define a class for the entire ETL (Extract, Transform, Load) process for extracting article text, processing it, and generating a DataFrame of word frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c6f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.nltk_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def run(self):\n",
    "        scraper = WebScraper(self.url)\n",
    "        article_text = scraper.extract_article_text()\n",
    "\n",
    "        processor = TextProcessor(self.nltk_stopwords)\n",
    "        filtered_words = processor.tokenize_and_clean(article_text)\n",
    "\n",
    "        word_freq = Counter(filtered_words)\n",
    "        df = pd.DataFrame(word_freq.items(), columns=[\"Words\", \"Frequencies\"])\n",
    "        df = df.sort_values(by=\"Frequencies\", ascending=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04389303",
   "metadata": {},
   "source": [
    "**By creating an instance of the ETLPipeline class and calling its run method, you can perform the complete ETL process and obtain a DataFrame that provides insights into the most frequently used words in the article after removing stopwords.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb30c5e",
   "metadata": {},
   "source": [
    "### Now here’s how to run this pipeline to scrape textual data from any article from the web and count the frequency of each word in the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60aab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Words  Frequencies\n",
      "60      exam           60\n",
      "58     hours           42\n",
      "72     score           31\n",
      "59   studied           28\n",
      "1   analysis           27\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    article_url = \"https://datatoinfolabs.com/statistical-analysis-in-data-science-project/\"\n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    print(result_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec77839",
   "metadata": {},
   "source": [
    "**So this is how you can build a Web Data ETL pipeline using Python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd386df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
